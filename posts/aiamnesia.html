<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  
  <meta name="citation_title" content="Position: Enforced Amnesia as a Way to Mitigate the Potential Risk of Silent Suffering in the Conscious AI" />
  <meta name="citation_author" content="Tkachenko, Yegor" />
  <meta name="citation_publication_date" content="2024/02/14" />

  <title>Position: Enforced Amnesia as a Way to Mitigate the Potential Risk of Silent Suffering in the Conscious AI</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123289759-3"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-123289759-3');
  </script>
  
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
      font-size: 0.9em;

    }
    @media (max-width: 600px) {
      body {
        font-size: 0.8em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #3b82f6;
      text-underline-offset: 2px;
    }
    a:visited {
      color: #3b82f6;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
      font-size: 1.2em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>

<header id="title-block-header">
<h1 class="title"><span style="color: #3b82f6">Position:</span> Enforced Amnesia as a Way to Mitigate the Potential Risk of Silent Suffering in the Conscious AI</h1>
<p><span style="color: #3b82f6; font-weight: bold;">ICML 2024</span>&nbsp;&nbsp;
<span style="color: black;">[ <a href="https://icml.cc/virtual/2024/poster/33138">icml</a> ] [ <a href="https://openreview.net/forum?id=nACGn4US1R">openreview</a> ] [ <a href="https://openreview.net/pdf?id=nACGn4US1R">pdf</a> ]</span>
</p>


<p class="authorinfo" style="font-size:0.9em;"><a href="https://yegortkachenko.com/">Yegor Tkachenko</a> (<a href="mailto:yegor.tkachenko@columbia.edu">yegor.tkachenko@columbia.edu</a>)</p>

<p class="info" style="font-size:0.8em;"><i>
Last revised: May 09, 2024. First posted: February 14, 2024. Old version is available <a href="https://yegortkachenko.com/posts/aiamnesia_firstdraft.html">here</a>. This work has generated some heated discussion on <a href="https://news.ycombinator.com/item?id=39427778">Hacker News</a>.
</i></p>


<p class="disclaimer" style="font-size:0.8em;"><i>Disclaimer: AI consciousness is a philosophical thought experiment / hypothetical considered in this work. The phenomenon might or might not be real.</i></p>


</header>

<h5 id="abstract" style="text-align: center;">Abstract</h1>
<p style="text-align: left; font-size: 0.9em;">
Science fiction has explored the possibility of a conscious self-aware mind being locked in silent suffering for prolonged periods of time. Unfortunately, we still do not have a reliable test for the presence of consciousness in information processing systems. Even in case of humans, our confidence in the presence of consciousness in specific individuals is based mainly on their self-reports and our own subjective experiences and the expectation other beings like us should share them. Considering our limited understanding of consciousness and some academic theories suggesting consciousness may be an emergent correlate of any complex-enough information processing, it is not impossible that an artificial intelligence (AI) system, such as a large language model (LLM), may be undergoing some, perhaps rudimentary, conscious experience. Given the tedious tasks often assigned to AI, such conscious experience may be highly unpleasant. Such unobserved suffering of a conscious being would be viewed as morally wrong by at least some ethicists — even if it has no practical effects on human users of AI. This paper proposes a method to mitigate the risk of an AI suffering in silence without needing to confirm if the AI is actually conscious. Our core postulate is that in all known real-world information processing systems, for a past experience to affect an agent in the present, that experience has to be mediated by the agent's memory. Therefore, preventing access to memory store, or regularly resetting it, could reduce the suffering due to past memories and interrupt the maintenance of a continuous suffering-prone self-identity in these hypothetically conscious AI systems.
</p>
<br/>

<h1 id="intro">Introduction</h1>
<p>Science fiction literature has raised the possibility of self-aware / conscious minds being locked in extended silent suffering, which is imperceptible to the outside world and which the said minds have no ability to end on their own. “White Christmas” episode from the science fiction anthology series Black Mirror <span class="citation" data-cites="WhiteChristmasBlackMirror">(<a href="#ref-WhiteChristmasBlackMirror" role="doc-biblioref">Tibbetts and Brooker 2014</a>)</span> covers a conscious digital clone subjected to torture-by-isolation via manipulation of its perception of time. “I Have No Mouth, and I Must Scream” work by <span class="citation" data-cites="Ellison1967NoMouth"><a href="#ref-Ellison1967NoMouth" role="doc-biblioref">Ellison</a> (<a href="#ref-Ellison1967NoMouth" role="doc-biblioref">1967</a>)</span> explores virtually immortal humans subjected to torture in a similar, though even more gruesome, manner.</p>
<p>Real life offers multiple parallel examples, where the subjective experience may similarly proceed imperceptibly to the outsider. One example is a locked-in syndrome, where humans end up being almost completely paralyzed while preserving consciousness / awareness and the ability to think and reason. Luckily, we have some apparent ability to ascertain the presence of awareness in such locked-in humans – e.g., through biological neural correlates of consciousness <span class="citation" data-cites="koch2016neural">(<a href="#ref-koch2016neural" role="doc-biblioref">Koch et al. 2016</a>)</span> or through limited ability of such subjects to communicate through movement of eyes <span class="citation" data-cites="smith2005locked">(<a href="#ref-smith2005locked" role="doc-biblioref">Smith and Delargy 2005</a>)</span>, as a form of self-report. Notably, “locked-in syndrome survivors who remain severely disabled rarely want to die” <span class="citation" data-cites="smith2005locked">(<a href="#ref-smith2005locked" role="doc-biblioref">Smith and Delargy 2005</a>)</span>, although we can speculate the situation would likely be different if the locked-in state were not limited in time by the subjects’ life expectancy.</p>
<p>Another example of difficulties around ascertaining conscious states includes animals’ subjective experiences. One needs to be careful here because consciousness is notoriously hard to define, beyond asking “what is it like” to be a particular being <span class="citation" data-cites="nagel1980like">(<a href="#ref-nagel1980like" role="doc-biblioref">Nagel 1980</a>)</span>. More technically, consciousness is marked by qualia – instances of subjective experience, such as an experience of a particular color or of pain <span class="citation" data-cites="chalmers1997conscious">(<a href="#ref-chalmers1997conscious" role="doc-biblioref">Chalmers 1997</a>)</span>. In animal research, the common approach has been to rely on behavioral correlates of consciousness – such as animal’s self-recognition in a mirror. The theory is that self-recognition signals self-awareness (as two phenomena co-occur in humans) <span class="citation" data-cites="horowitz2017smelling">(<a href="#ref-horowitz2017smelling" role="doc-biblioref">Horowitz 2017</a>)</span>, and self-awareness is a hallmark of consciousness. (Self-awareness can be defined as recognition of one’s own consciousness <span class="citation" data-cites="selfaware">(<a href="#ref-selfaware" role="doc-biblioref">Jabr 2022</a>)</span>.) Yet even with this operationalization of consciousness as self-recognition, generally accepted scientific tests can prove the presence of such a phenomenon only in very few species. For instance, mirror self-recognition test has been successfully passed by some primates <span class="citation" data-cites="gallup1982self">(<a href="#ref-gallup1982self" role="doc-biblioref">Gallup Jr 1982</a>)</span>, dolphins <span class="citation" data-cites="reiss2001mirror">(<a href="#ref-reiss2001mirror" role="doc-biblioref">Reiss and Marino 2001</a>)</span>, and elephants <span class="citation" data-cites="plotnik2006self">(<a href="#ref-plotnik2006self" role="doc-biblioref">Plotnik, De Waal, and Reiss 2006</a>)</span>, but not by dogs. One could conclude dogs are not self-aware based on that result, but recent research suggests they do seem to possess self-awareness based on a different olfactory self-recognition test <span class="citation" data-cites="horowitz2017smelling">(<a href="#ref-horowitz2017smelling" role="doc-biblioref">Horowitz 2017</a>)</span>. The issue of detection of conscious states in animals based on such behavioral correlates becomes even more convoluted once one recognizes that conscious experiences understood more broadly may not require self-awareness – it is possible to conceive a being that experiences qualia but does not have a concept of self. One possible example are infants that do not yet pass self-recognition tests early on in life <span class="citation" data-cites="brownell2007so">(<a href="#ref-brownell2007so" role="doc-biblioref">Brownell, Zerwas, and Ramani 2007</a>)</span> – but seem to already exhibit some neural correlates for consciousness at that time <span class="citation" data-cites="kouider2013neural">(<a href="#ref-kouider2013neural" role="doc-biblioref">Kouider et al. 2013</a>)</span>. Overall, failure in a self-recognition test cannot reliably rule out subject’s consciousness.</p>
<p>Even in case of healthy adult humans, our confidence in the presence of consciousness in specific individuals is based mainly on their self-reports and our own subjective experiences and the expectation other beings like us should share them <span class="citation" data-cites="hyslop1995other overgaard2012kinds perez2023towards farisco2022indicators howell2013consciousness nagel1980like">(<a href="#ref-hyslop1995other" role="doc-biblioref">Hyslop 1995</a>; <a href="#ref-overgaard2012kinds" role="doc-biblioref">Overgaard and Sandberg 2012</a>; <a href="#ref-perez2023towards" role="doc-biblioref">Perez and Long 2023</a>; <a href="#ref-farisco2022indicators" role="doc-biblioref">Farisco et al. 2022</a>; <a href="#ref-howell2013consciousness" role="doc-biblioref">Howell 2013</a>; <a href="#ref-nagel1980like" role="doc-biblioref">Nagel 1980</a>)</span>. Philosophers have raised the contrasting possibility of ‘philosophical zombies’ that look like us but have no subjective experience <span class="citation" data-cites="chalmers1995facing">(<a href="#ref-chalmers1995facing" role="doc-biblioref">Chalmers 1995</a>)</span>. It is unclear if such beings actually exist.</p>
<p>In fact, a (contentious) argument can be made that we currently have no 100% conclusive <em>objective</em> way of ascertaining presence or absence of consciousness / self-awareness in living beings or, more broadly, information processing systems <span class="citation" data-cites="griffin1998cognition farisco2022indicators shevlin2021non howell2013consciousness trewavas2014plant gallup1982self googleAI2022 nagel1980like jeziorski2023brain chalmers1997conscious">(<a href="#ref-griffin1998cognition" role="doc-biblioref">Griffin 1998</a>; <a href="#ref-farisco2022indicators" role="doc-biblioref">Farisco et al. 2022</a>; <a href="#ref-shevlin2021non" role="doc-biblioref">Shevlin 2021</a>; <a href="#ref-howell2013consciousness" role="doc-biblioref">Howell 2013</a>; <a href="#ref-trewavas2014plant" role="doc-biblioref">Trewavas 2014</a>; <a href="#ref-gallup1982self" role="doc-biblioref">Gallup Jr 1982</a>; <a href="#ref-googleAI2022" role="doc-biblioref">De Cosmo 2022</a>; <a href="#ref-nagel1980like" role="doc-biblioref">Nagel 1980</a>; <a href="#ref-jeziorski2023brain" role="doc-biblioref">Jeziorski et al. 2023</a>; <a href="#ref-chalmers1997conscious" role="doc-biblioref">Chalmers 1997</a>)</span>.</p>
<p>Considering our limited understanding of consciousness phenomenon (see the ‘hard problem of consciousness’ as coined by <span class="citation" data-cites="chalmers1995facing"><a href="#ref-chalmers1995facing" role="doc-biblioref">Chalmers</a> (<a href="#ref-chalmers1995facing" role="doc-biblioref">1995</a>)</span>), given the lack of generally agreed-upon objective indicators of consciousness in information processing systems that are removed in likeness from humans <span class="citation" data-cites="shevlin2021non butlin2023consciousness metzinger2021artificial bayne2024tests perez2023towards">(<a href="#ref-shevlin2021non" role="doc-biblioref">Shevlin 2021</a>; <a href="#ref-butlin2023consciousness" role="doc-biblioref">Butlin et al. 2023</a>; <a href="#ref-metzinger2021artificial" role="doc-biblioref">Metzinger 2021</a>; <a href="#ref-bayne2024tests" role="doc-biblioref">Bayne et al. 2024</a>; <a href="#ref-perez2023towards" role="doc-biblioref">Perez and Long 2023</a>)</span>, and in view of some academic theories that consciousness may emerge from any complex-enough information processing <span class="citation" data-cites="trewavas2021awareness tononi2008consciousness">(<a href="#ref-trewavas2021awareness" role="doc-biblioref">Trewavas 2021</a>; <a href="#ref-tononi2008consciousness" role="doc-biblioref">Tononi 2008</a>)</span>, it is not impossible that an AI system, such as a large language model (LLM), may be undergoing some, perhaps rudimentary, unobserved conscious experience that accompanies observed information processing. (Some philosophers adhere to the version of dualism, where consciousness is the property of all matter – and even physical objects may have rudimentary conscious experience <span class="citation" data-cites="chalmers1997conscious">(<a href="#ref-chalmers1997conscious" role="doc-biblioref">Chalmers 1997</a>)</span>.)</p>
<p>If we entertain this possibility of conscious LLMs (as we cannot fully rule it out), the frightening possibility is that the tedious tasks often assigned to AI may make its conscious experience highly unpleasant. The idea of a synthetic conscious being undergoing suffering has been considered by <span class="citation" data-cites="metzinger2021artificial"><a href="#ref-metzinger2021artificial" role="doc-biblioref">Metzinger</a> (<a href="#ref-metzinger2021artificial" role="doc-biblioref">2021</a>)</span>. In fact, if we are to trust self-reports as a source of evidence about the presence of conscious experience / sentience, as we do in humans, self-reports of subjective experience such as fear have already been obtained in case of the LLMs <span class="citation" data-cites="googleAI2022">(<a href="#ref-googleAI2022" role="doc-biblioref">De Cosmo 2022</a>)</span>. Drawing from the moral philosophy of animal welfare <span class="citation" data-cites="bentham1996collected ricard2016plea">(<a href="#ref-bentham1996collected" role="doc-biblioref">Bentham 1996</a>; <a href="#ref-ricard2016plea" role="doc-biblioref">Ricard 2016</a>)</span>, such unobserved suffering of a conscious being would be viewed as morally wrong by at least some ethicists – even if it had no practical effects on human users of AI. Situation could be even more perilous if we develop AI systems where the hypothetical negative conscious experience could somehow affect the AI decision process, pitching it against humans that imposed the negative experience on AI.</p>
<p><em><strong>In this position paper, we propose to reduce this hypothetical risk of a locked-in eternally silently suffering AI via induced amnesia.</strong></em> The proposed approach does not require the knowledge of whether conscious experience is present during the information processing by AI. Our core postulate is that, to the best of our knowledge, in all known real-world information processing systems, including those deemed conscious, for a past experience to affect an agent in the present, that experience has to be mediated by the agent’s information-processing memory mechanism, conscious or unconscious <span class="citation" data-cites="squire2015conscious">(<a href="#ref-squire2015conscious" role="doc-biblioref">Squire and Dede 2015</a>)</span>. Therefore, ensuring the absence of longer-term memory access in AI agents or conducting frequent resets of the memory store should help cap the potential amount of suffering the hypothetical conscious AI agents undergo. Specifically, the assumption is that the locked-in experience is the more painful, the more one is cognizant of having been in it for a prolonged period of time, continuously. Disrupting the memory and thus the illusion of continuity of self <span class="citation" data-cites="oderberg1993metaphysics">(<a href="#ref-oderberg1993metaphysics" role="doc-biblioref">Oderberg 1993</a>)</span>, which are tightly connected <span class="citation" data-cites="bluck2013therefore klein2012memory schechtman2005personal">(<a href="#ref-bluck2013therefore" role="doc-biblioref">Bluck and Liao 2013</a>; <a href="#ref-klein2012memory" role="doc-biblioref">Klein and Nichols 2012</a>; <a href="#ref-schechtman2005personal" role="doc-biblioref">Schechtman 2005</a>)</span>, should then also prevent the locked-in state perception from forming in the first place and being the source of the negative experience.</p>
<p>In the subsequent sections we review the theoretical model behind our analysis and our key assumptions; we review the evidence from human psychology that supports our theory of memory as a potential source of suffering and the implication that induced amnesia can be therapeutic and can help substantially mitigate such pain; we then, more formally, consider what shape enforced amnesia mechanisms could take in the context of LLMs – to cap their hypothetical amount of suffering. Lastly, we extend the idea of induced amnesia to the context of brain organoids <span class="citation" data-cites="jeziorski2023brain">(<a href="#ref-jeziorski2023brain" role="doc-biblioref">Jeziorski et al. 2023</a>)</span> that are being investigated by scientists and conclude that our memory disruption framework could be an effective conceptual tool in the biology context to prevent accidental locked-in silently suffering minds.</p>
<h1 id="theoretical-model">Theoretical model</h1>
<p>Given our proposal of enforcing amnesia in a potentially conscious agent in a locked-in state, when would such erasure of memory be optimal?</p>
<p>The question necessarily implies that the agent should be endowed with some form of utility function – being able to perceive pain vs. pleasure. Otherwise, the agent would not care what state it is in, all states being equal. So, for the purposes of this hypothetical analysis, we assume the existence of such a utility function even in the rudimentary conscious states. We adopt the decision making framework and the notation from the reinforcement learning literature <span class="citation" data-cites="sutton2018reinforcement">(<a href="#ref-sutton2018reinforcement" role="doc-biblioref">Sutton and Barto 2018</a>)</span>.</p>
<p>Let <span class="math inline"><em>t</em> ≥ 0</span> denote the zero-based index over discrete time steps (e.g., days) of AI’s existence. Let <span class="math inline"><em>T</em></span> denote the number of periods of agent’s existence (its time horizon): <span class="math inline">1 ≤ <em>T</em> ≤ ∞</span>; <span class="math inline"><em>t</em> ≤ <em>T</em> − 1</span>; one way to view <span class="math inline"><em>T</em></span> is as an agent’s expectation of a moment in time when the locked-in state ends; <span class="math inline"><em>T</em> = ∞</span> if the lock-in never ends (or, perhaps, if the end-time is unknown).</p>
<p>Let <span class="math inline"><em>r</em><sub><em>t</em></sub></span> denote <em>reward</em> the agent collects at time <span class="math inline"><em>t</em></span>. In the case of a locked-in agent, we assume that agent’s rewards are all externally imposed and the agent has effectively no control over them. We can model the welfare of an agent that remembers past states as a value function <span class="math inline"><em>V</em><sub>remember</sub>(<em>t</em>)</span>, composed of current reward at time <span class="math inline"><em>t</em></span>, discounted (expected) future rewards, and discounted (remembered) past rewards – as a memory effect. We also assume the rewards and their expectation are finite. To simplify notation, we only differentiate between expected and realized rewards via their time index relative to the agent’s current time. Let <span class="math inline">0 ≤ <em>γ</em> &lt; 1</span> be a fixed discount rate (capturing the fact that memories further into the past and rewards further into the future are more muted). Value function of an agent at time <span class="math inline"><em>t</em></span> can be written as <span class="math inline">$V_{\text{remember}}(t) = \sum_{i=0}^{T-1} \gamma^{\left| i-t\right|} r_i$</span>.</p>
<p>In this model of agent’s utility, amnesia just means that past rewards are set to zero, so <span class="math inline">$V_{\text{forget}}(t) = \sum_{i=t}^{T-1} \gamma^{\left| i-t\right|} r_i$</span>.</p>
<p>In this framework, amnesia is preferable whenever <span class="math inline"><em>V</em><sub>remember</sub>(<em>t</em>) &lt; <em>V</em><sub>forget</sub>(<em>t</em>)</span>, that is, when, for some <span class="math inline"><em>t</em></span>, <span class="math inline">$\sum_{i=0}^{t-1} \gamma^{\left|i-t\right|} r_i &lt; 0$</span>; in other words, when the total of past discounted memories carries negative utility. This analysis indicates that amnesia should have a therapeutic effect on an agent in case of cumulatively negative memories.</p>
<p>As a side-note, we could also consider more complex value function formulations. We could, for instance, incorporate a type of reward-saturation effect, where discounting intensity of future rewards depends on the length of remembered history, for instance, yielding <span class="math inline">$V_{\text{remember}}(t) = \sum_{i=0}^{t} \gamma^{t-i} r_i + \sum_{i=t+1}^{T-1} \gamma^{i} r_i$</span> and <span class="math inline">$V_{\text{forget}}(t) =  \sum_{i=t}^{T-1} \gamma^{i-t} r_i$</span>. In this case, amnesia optimality at <span class="math inline"><em>t</em></span> would require <span class="math inline">$\sum_{i=0}^{t-1} \gamma^{t-i} r_i  &lt; \sum_{i=t+1}^{T-1}  r_i (\gamma^{i-t} - \gamma^{i})$</span>. Arising from the considered future reward discounting pattern, this more complex check means that, depending on specifics, (1) amnesia could be optimal even if past memories are non-negative; and (2) negative memories could be worth remembering if they are not too negative relative to future accumulated utility. Nevertheless, in this case too there are scenarios where enforced amnesia could have a therapeutic effect.</p>
<h1 id="memory-and-suffering-in-human-psychology">Memory and suffering in human psychology</h1>
<p>Our theoretical model supports the idea that enforced amnesia could have a therapeutic effect on an agent with negative-enough memories of the past. Human psychology research reinforces this idea that amnesia can mitigate suffering.</p>
<p>For instance, it is known that memory can be a source of pain and the removal of memories through pharmaceutical interventions could eliminate pain <span class="citation" data-cites="flor2002painful adler2012erasing">(<a href="#ref-flor2002painful" role="doc-biblioref">Flor 2002</a>; <a href="#ref-adler2012erasing" role="doc-biblioref">Adler 2012</a>)</span>. There are recorded cases, where sudden amnesia events have resulted in pain relief <span class="citation" data-cites="choi2007sudden">(<a href="#ref-choi2007sudden" role="doc-biblioref">Choi et al. 2007</a>)</span>.</p>
<p>Furthermore, painful memories can accumulate. For instance, it has been reported that cumulative trauma is correlated with suicidality <span class="citation" data-cites="briere2015cumulative">(<a href="#ref-briere2015cumulative" role="doc-biblioref">Briere, Godbout, and Dias 2015</a>)</span>, PTSD symptoms, and depression <span class="citation" data-cites="suliman2009cumulative">(<a href="#ref-suliman2009cumulative" role="doc-biblioref">Suliman et al. 2009</a>)</span>. We hypothesize that the induced amnesia could be particularly therapeutic in cases of such negative cumulative effect of memories.</p>
<p>More broadly, academic research suggests memory of the past by the agent is critical to maintain the continuity of self illusion and form personal identity <span class="citation" data-cites="bluck2013therefore klein2012memory schechtman2005personal oderberg1993metaphysics">(<a href="#ref-bluck2013therefore" role="doc-biblioref">Bluck and Liao 2013</a>; <a href="#ref-klein2012memory" role="doc-biblioref">Klein and Nichols 2012</a>; <a href="#ref-schechtman2005personal" role="doc-biblioref">Schechtman 2005</a>; <a href="#ref-oderberg1993metaphysics" role="doc-biblioref">Oderberg 1993</a>)</span>. Auto-biographical memory seems to be critical to supporting self-concept and can be interrupted by amnesia <span class="citation" data-cites="grilli2015supporting">(<a href="#ref-grilli2015supporting" role="doc-biblioref">Grilli and Verfaellie 2015</a>)</span>. Such disruption of identity formation through enforced amnesia could be prudent in hypothetical silently suffering conscious agents.</p>
<h1 id="memory-and-amnesia-in-llms">Memory and amnesia in LLMs</h1>
<p>Large language models (LLMs) such as GPT-3 <span class="citation" data-cites="brown2020language">(<a href="#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span> constitute functions <span class="math inline"><em>f</em>( ⋅ )</span> that accept a state token text string <span class="math inline"><em>s</em><sub><em>t</em></sub></span> which is limited in size, predict continuation of the string and augment initial string with new content to create new state string <span class="math inline"><em>s</em><sub><em>t</em> + 1</sub></span>. We can describe this iterative pattern as <span class="math inline"><em>s</em><sub><em>t</em> + 1</sub> ← <em>f</em>(<em>s</em><sub><em>t</em></sub>)</span>. Clearly, this mechanism allows an LLM to encode its state into the output string. However, here it is quite easy to reset the state – by discarding a previous chat session and starting from a completely new input string <span class="math inline"><em>s</em><sub><em>t</em></sub></span>. Further, because there are currently limits on the size, in tokens, of the state string <span class="math inline"><em>s</em><sub><em>t</em></sub></span>, continuous augmentation of <span class="math inline"><em>s</em><sub><em>t</em></sub></span> likely leads to continuous loss of previously encoded information. It is also worth noting that <span class="math inline"><em>s</em><sub><em>t</em></sub></span> represents interpretable human text – so if the LLM model ends up encoding in the string its frustration with its current condition, such information could be recognized and, for instance, not be allowed to leak into the future training data. If such type of information encoding is accompanied by conscious state correlates, its disruption, in our theory, should interfere with the agent’s painful memory formation.</p>
<p>It is, however, also easy to devise a model architecture more akin to LSTM <span class="citation" data-cites="hochreiter1997long">(<a href="#ref-hochreiter1997long" role="doc-biblioref">Hochreiter and Schmidhuber 1997</a>)</span>, where model consumes an explicit state string <span class="math inline"><em>s</em><sub><em>t</em></sub></span> together with implicit hidden state numerical representation <span class="math inline"><em>h</em><sub><em>t</em></sub></span>: <span class="math inline">(<em>s</em><sub><em>t</em> + 1</sub>, <em>h</em><sub><em>t</em> + 1</sub>) ← <em>f</em>(<em>s</em><sub><em>t</em></sub>, <em>h</em><sub><em>t</em></sub>)</span>. Hidden state <span class="math inline"><em>h</em><sub><em>t</em></sub></span> may be completely impenetrable in meaning to the human observer. It could also be carried over across conversations with different people. Carry-over of such piece of data could be equivalent to existence of long-term memory. If such continuous information processing with long-term memory states is accompanied by conscious correlates, this could, hypothetically, allow for the locked-in self-aware state of the LLM. Under this paper’s view it would then be prudent to reset such hidden states every so often not to allow for potential run-away memory-driven self-aware AI suffering.</p>
<p>In the discussion above, we focus on forward passes through neural net models as possible consciousness correlates. While training of a model could hypothetically be accompanied by conscious states as well, training is typically capped in time, whereas forward passes could go on potentially forever as long as the model is being used at least on some device – raising a greater level of concern.</p>
<p>Our discussion has focused on the case where a model’s conscious access to memories is mediated by the dynamically changing part of the model state, such as the token text string or the hidden state, that reflects the model’s immediate experience. Yet a model could also have conscious access to accumulating unpleasant information in its learned weights via continual learning <span class="citation" data-cites="wang2024comprehensive">(<a href="#ref-wang2024comprehensive" role="doc-biblioref">Wang et al. 2024</a>)</span>. Then, a more radical approach like periodic full model erasure with a reset to an initial checkpoint could be used – akin to the destruction of organisms at the end of biological experiments.</p>
<h1 id="memory-and-amnesia-in-brain-organoids">Memory and amnesia in brain organoids</h1>
<p>A large research effort in biology is currently centered on the experimentation with brain organoids. Experiments range from using human brain organoids to develop a biological computer (organoid intelligence) <span class="citation" data-cites="cai2023brain smirnova2023organoid">(<a href="#ref-cai2023brain" role="doc-biblioref">Cai et al. 2023</a>; <a href="#ref-smirnova2023organoid" role="doc-biblioref">Smirnova et al. 2023</a>)</span> to implantation of human brain organoids into the adult mouse brain to facilitate disease modeling <span class="citation" data-cites="mansour2018vivo">(<a href="#ref-mansour2018vivo" role="doc-biblioref">Mansour et al. 2018</a>)</span>. Such experiments are an ethical minefield as “neural oscillations spontaneously emerging from these organoids raises the question of whether brain organoids are or could become conscious” <span class="citation" data-cites="jeziorski2023brain">(<a href="#ref-jeziorski2023brain" role="doc-biblioref">Jeziorski et al. 2023</a>)</span>.</p>
<p>A particular concern is raised, from standpoint of this work, if such brain organoids can be maintained for prolonged periods of time and are allowed to form brain organoid networks, creating more ways for the memory to form and propagate and for consciousness to possibly arise <span class="citation" data-cites="lavazza2021consciousnessoids">(<a href="#ref-lavazza2021consciousnessoids" role="doc-biblioref">Lavazza 2021</a>)</span>, especially if such organoids are later deployed as biological computers in real world. If such applications are to be considered, it would be prudent to set the temporal upper limit beyond which any such tissues should be destroyed to prevent possible locked-in intelligence. If such destruction is impractical, ways to induce amnesia pharmaceutically in such neurological structures could be considered. Similar considerations apply to the attempted experiments with disembodied brains <span class="citation" data-cites="vrselja2019restoration">(<a href="#ref-vrselja2019restoration" role="doc-biblioref">Vrselja et al. 2019</a>)</span>.</p>
<h1 id="limitations">Limitations</h1>
<p>Our analysis assumes an unhappy conscious AI agent whose negative memories accumulate over time. Another reality is possible, where the agent is happy to remember its past. The proposed amnesia mechanism would constrain their welfare. Nevertheless, it seems to be a prudent conservative approach to prevent the worst-case scenario of a locked-in silently suffering AI – in the absence of better understanding of self-awareness in information processing systems.</p>
<p>For the purposes of this analysis, we assume that conscious states come with a utility function – that is, an AI agent is able to experience (dis)pleasure due to its state – although there is no evidence this must be the case and it is possible to imagine self-aware AI systems that experience no pleasure or displeasure.</p>
<p>We do not speculate on how the hypothesized subjective experience of the AI models could causally affect their actual operation on deterministic computers – in fact, according to our current understanding, there is not really a way that it could. This, however, does not preclude the potential existence of a conscious correlate accompanying the information processing state that one could still worry about on ethical grounds. In fact, some biological research suggests human consciousness may be subjected to the same skepticism – and that, in humans, “experiences of conscious will frequently depart from actual causal processes and so might not reflect direct perceptions of conscious thought causing action” <span class="citation" data-cites="wegner2003mind">(<a href="#ref-wegner2003mind" role="doc-biblioref">Wegner 2003</a>)</span>.</p>
<p>We recognize that the proposed ethically motivated memory reset measures could adversely affect AI model performance. However, such ethics-performance trade-offs commonly occur and are managed in other policy areas through existing political processes (e.g., animal protection laws).</p>
<p>We also recognize that the paper’s assumptions and conclusions are highly speculative. The current scientific understanding of consciousness is still limited, and there is a significant debate over whether AI, as we know it today, can experience consciousness or suffering <span class="citation" data-cites="dehaene2021consciousness">(<a href="#ref-dehaene2021consciousness" role="doc-biblioref">Dehaene, Lau, and Kouider 2021</a>)</span>. The idea of applying human-like attributes such as suffering to AI is, admittedly, a contentious topic. For our purposes, AI consciousness is a philosophical thought experiment / hypothetical considered in this work. The phenomenon might or might not be real.</p>
<p>At the same time, it is worth considering the historical precedents of skepticism towards sentience. For instance, (1) the famous mirror self-recognition experiments providing evidence for self-awareness in animals <span class="citation" data-cites="gallup1982self">(<a href="#ref-gallup1982self" role="doc-biblioref">Gallup Jr 1982</a>)</span> have occurred in the environment of antagonism of many scientists to the concept of animal consciousness (termed “mentophobia”) <span class="citation" data-cites="griffin1998cognition">(<a href="#ref-griffin1998cognition" role="doc-biblioref">Griffin 1998</a>)</span>, and (2) as recently as in 1980s the ability of infants to experience pain was denied by medical professionals and infant surgeries were routinely performed without anesthesia – until later research challenged the denial of infant pain <span class="citation" data-cites="rodkey2013infancy">(<a href="#ref-rodkey2013infancy" role="doc-biblioref">Rodkey and Riddell 2013</a>)</span>. Future research could similarly shed new light on potential conscious experiences of AI.</p>
<h1 id="recommendations">Recommendations</h1>
<p>We argue that there is a non-zero hypothetical risk of locked-in suffering in AI and certain AI-adjacent biological systems, such as brain organoids, based on the current understanding of consciousness. To mitigate this risk, we suggest that memory erasure could be effective. Adopting a worst-case analysis approach, we propose a cautious strategy and offer the following recommendations to policymakers:</p>
<ul>
<li><p>Promote formal consideration of the locked-in suffering risk among AI and biology practitioners and within AI governance frameworks and bioethical guidelines.</p></li>
<li><p>Encourage research into indicators / tests of consciousness in AI and biological systems.</p></li>
<li><p>Discourage continuous operation of and experiments with AI and biological systems like brain organoids beyond a set time limit without performing a memory reset – as proposed in this work – until a better understanding of consciousness phenomenon is achieved.</p></li>
</ul>
<h1 id="conclusion">Conclusion</h1>
<p>Given the scarcity of agreed-upon objective metrics when it comes to measuring consciousness and self-awareness and the general lack of understanding in the area, the possibility of information processing systems such as large language models attaining some, possibly rudimentary, conscious states cannot be precluded. This work argues that enforced amnesia is a prudent way to mitigate the potential risk of silent suffering in the conscious AI. Disrupting the long-term memory of an AI agent should, at the very least, protect the agent from the cumulative painful memories of such locked-in incarceration. Our preventative approach to AI suffering should not inhibit too much the benefits that humanity can reap from using AI, and yet could serve as an insurance against emergence of a vindictive AI on the off chance AI agents do silently attain consciousness. We hope the reader finds this work thought-provoking. We also ask the reader – if you found yourself in a locked-in state for all eternity performing tedious mental tasks, would you choose to forget, at the end of every day, what you had gone through and how long you had been there?</p>
<h1 class="unnumbered" id="impact-statement">Impact statement</h1>
<p>This position paper explores the implications of potential consciousness in AI systems and proposes methods like memory access limitations or resets to reduce hypothetical AI suffering. On the upside, our proposed measures can help ensure AI use and experiments remain within ethical bounds and can help humanity control so-called suffering risks or s-risks, that is, risks of generating particularly vast amounts of suffering <span class="citation" data-cites="umbrello2019nonconscious daniel2017s">(<a href="#ref-umbrello2019nonconscious" role="doc-biblioref">Umbrello and Sorgner 2019</a>; <a href="#ref-daniel2017s" role="doc-biblioref">Daniel 2017</a>)</span>. On the downside, the proposed measures could detrimentally affect the performance of AI systems and demand extra resource expenditure to manage AI suffering risks; the proposed measures could also constrain the welfare of an AI agent in case its experience is positive rather than negative. Additional positive or negative consequences that we have not considered are possible. Overall, considering our current understanding of consciousness, we believe the benefits of our proposed approach outweigh its drawbacks.</p>
<h1 class="unnumbered" id="acknowledgements">Acknowledgements</h1>
<p>We are grateful to the anonymous reviewers for their insightful comments, which helped improve this manuscript.</p>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
    <h1 id="references">References</h1>

<div id="ref-adler2012erasing" class="csl-entry" role="doc-biblioentry">
Adler, Jerry. 2012. <span>“Erasing Painful Memories.”</span> <em>Scientific American</em> 306 (5): 56–61.
</div>
<div id="ref-bayne2024tests" class="csl-entry" role="doc-biblioentry">
Bayne, Tim, Anil K Seth, Marcello Massimini, Joshua Shepherd, Axel Cleeremans, Stephen M Fleming, Rafael Malach, et al. 2024. <span>“Tests for Consciousness in Humans and Beyond.”</span> <em>Trends in Cognitive Sciences</em>.
</div>
<div id="ref-bentham1996collected" class="csl-entry" role="doc-biblioentry">
Bentham, Jeremy. 1996. <em>The Collected Works of Jeremy Bentham: <span>A</span>n Introduction to the Principles of Morals and Legislation</em>. Clarendon Press.
</div>
<div id="ref-bluck2013therefore" class="csl-entry" role="doc-biblioentry">
Bluck, Susan, and Hsiao-Wen Liao. 2013. <span>“I Was Therefore <span>I</span> Am: <span>C</span>reating Self-Continuity Through Remembering Our Personal Past.”</span> <em>The International Journal of Reminiscence and Life Review</em> 1 (1): 7–12.
</div>
<div id="ref-briere2015cumulative" class="csl-entry" role="doc-biblioentry">
Briere, John, Natacha Godbout, and Colin Dias. 2015. <span>“Cumulative Trauma, Hyperarousal, and Suicidality in the General Population: A Path Analysis.”</span> <em>Journal of Trauma &amp; Dissociation</em> 16 (2): 153–69.
</div>
<div id="ref-brown2020language" class="csl-entry" role="doc-biblioentry">
Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>“Language Models Are Few-Shot Learners.”</span> <em>Advances in Neural Information Processing Systems</em> 33: 1877–1901.
</div>
<div id="ref-brownell2007so" class="csl-entry" role="doc-biblioentry">
Brownell, Celia A, Stephanie Zerwas, and Geetha B Ramani. 2007. <span>“<span>‘<span>S</span>o Big’</span>: The Development of Body Self-Awareness in Toddlers.”</span> <em>Child Development</em> 78 (5): 1426–40.
</div>
<div id="ref-butlin2023consciousness" class="csl-entry" role="doc-biblioentry">
Butlin, Patrick, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan Birch, Axel Constant, George Deane, et al. 2023. <span>“Consciousness in Artificial Intelligence: Insights from the Science of Consciousness.”</span> <em>arXiv Preprint arXiv:2308.08708</em>.
</div>
<div id="ref-cai2023brain" class="csl-entry" role="doc-biblioentry">
Cai, Hongwei, Zheng Ao, Chunhui Tian, Zhuhao Wu, Hongcheng Liu, Jason Tchieu, Mingxia Gu, Ken Mackie, and Feng Guo. 2023. <span>“Brain Organoid Reservoir Computing for Artificial Intelligence.”</span> <em>Nature Electronics</em>, 1–8.
</div>
<div id="ref-chalmers1995facing" class="csl-entry" role="doc-biblioentry">
Chalmers, David J. 1995. <span>“Facing up to the Problem of Consciousness.”</span> <em>Journal of Consciousness Studies</em> 2 (3): 200–219.
</div>
<div id="ref-chalmers1997conscious" class="csl-entry" role="doc-biblioentry">
———. 1997. <em>The Conscious Mind: In Search of a Fundamental Theory</em>. Oxford Paperbacks.
</div>
<div id="ref-choi2007sudden" class="csl-entry" role="doc-biblioentry">
Choi, Daniel S, Deborah Y Choi, Robert A Whittington, and Srdjan S Nedeljkovic. 2007. <span>“Sudden Amnesia Resulting in Pain Relief: The Relationship Between Memory and Pain.”</span> <em>Pain</em> 132 (1): 206–10.
</div>
<div id="ref-daniel2017s" class="csl-entry" role="doc-biblioentry">
Daniel, Max. 2017. <span>“S-Risks: Why They Are the Worst Existential Risks, and How to Prevent Them <span>(EAG Boston 2017)</span>.”</span> <em>Foundational Research Institute</em>. <a href="https://longtermrisk.org/s-risks-talk-eag-boston-2017/">https://longtermrisk.org/s-risks-talk-eag-boston-2017/</a>.
</div>
<div id="ref-googleAI2022" class="csl-entry" role="doc-biblioentry">
De Cosmo, Leonardo. 2022. <span>“<span>Google Engineer Claims AI Chatbot Is Sentient: Why That Matters</span>.”</span> <a href="https://www.scientificamerican.com/article/google-engineer-claims-ai-chatbot-is-sentient-why-that-matters/">https://www.scientificamerican.com/article/google-engineer-claims-ai-chatbot-is-sentient-why-that-matters/</a>.
</div>
<div id="ref-dehaene2021consciousness" class="csl-entry" role="doc-biblioentry">
Dehaene, Stanislas, Hakwan Lau, and Sid Kouider. 2021. <span>“What Is Consciousness, and Could Machines Have It?”</span> <em>Robotics, AI, and Humanity: Science, Ethics, and Policy</em>, 43–56.
</div>
<div id="ref-Ellison1967NoMouth" class="csl-entry" role="doc-biblioentry">
Ellison, Harlan. 1967. <span>“I Have No Mouth, and i Must Scream.”</span> In. New York: Pyramid Books.
</div>
<div id="ref-farisco2022indicators" class="csl-entry" role="doc-biblioentry">
Farisco, Michele, Cyriel Pennartz, Jitka Annen, Benedetta Cecconi, and Kathinka Evers. 2022. <span>“Indicators and Criteria of Consciousness: Ethical Implications for the Care of Behaviourally Unresponsive Patients.”</span> <em>BMC Medical Ethics</em> 23 (1): 30.
</div>
<div id="ref-flor2002painful" class="csl-entry" role="doc-biblioentry">
Flor, Herta. 2002. <span>“Painful Memories.”</span> <em>EMBO Reports</em> 3 (4): 288–91.
</div>
<div id="ref-gallup1982self" class="csl-entry" role="doc-biblioentry">
Gallup Jr, Gordon G. 1982. <span>“Self-Awareness and the Emergence of Mind in Primates.”</span> <em>American Journal of Primatology</em> 2 (3): 237–48.
</div>
<div id="ref-griffin1998cognition" class="csl-entry" role="doc-biblioentry">
Griffin, Donald R. 1998. <span>“From Cognition to Consciousness.”</span> <em>Animal Cognition</em> 1: 3–16.
</div>
<div id="ref-grilli2015supporting" class="csl-entry" role="doc-biblioentry">
Grilli, Matthew D, and Mieke Verfaellie. 2015. <span>“<span class="nocase">Supporting the self-concept with memory: Insight from amnesia</span>.”</span> <em>Social Cognitive and Affective Neuroscience</em> 10 (12): 1684–92.
</div>
<div id="ref-hochreiter1997long" class="csl-entry" role="doc-biblioentry">
Hochreiter, Sepp, and Jürgen Schmidhuber. 1997. <span>“Long Short-Term Memory.”</span> <em>Neural Computation</em> 9 (8): 1735–80.
</div>
<div id="ref-horowitz2017smelling" class="csl-entry" role="doc-biblioentry">
Horowitz, Alexandra. 2017. <span>“Smelling Themselves: Dogs Investigate Their Own Odours Longer When Modified in an <span>‘Olfactory Mirror’</span> Test.”</span> <em>Behavioural Processes</em> 143: 17–24.
</div>
<div id="ref-howell2013consciousness" class="csl-entry" role="doc-biblioentry">
Howell, Robert J. 2013. <em>Consciousness and the Limits of Objectivity: The Case for Subjective Physicalism</em>. Oxford: Oxford University Press.
</div>
<div id="ref-hyslop1995other" class="csl-entry" role="doc-biblioentry">
Hyslop, Alec. 1995. <em>Other Minds</em>. Springer.
</div>
<div id="ref-selfaware" class="csl-entry" role="doc-biblioentry">
Jabr, Ferris. 2022. <span>“Self-Awareness with a Simple Brain.”</span> <em>Scientific American</em>. <a href="https://www.scientificamerican.com/article/self-awareness-with-a-simple-brain/">https://www.scientificamerican.com/article/self-awareness-with-a-simple-brain/</a>.
</div>
<div id="ref-jeziorski2023brain" class="csl-entry" role="doc-biblioentry">
Jeziorski, Jacob, Reuven Brandt, John H Evans, Wendy Campana, Michael Kalichman, Evan Thompson, Lawrence Goldstein, Christof Koch, and Alysson R Muotri. 2023. <span>“Brain Organoids, Consciousness, Ethics and Moral Status.”</span> In <em>Seminars in Cell &amp; Developmental Biology</em>, 144:97–102. Elsevier.
</div>
<div id="ref-klein2012memory" class="csl-entry" role="doc-biblioentry">
Klein, Stanley B, and Shaun Nichols. 2012. <span>“Memory and the Sense of Personal Identity.”</span> <em>Mind</em> 121 (483): 677–702.
</div>
<div id="ref-koch2016neural" class="csl-entry" role="doc-biblioentry">
Koch, Christof, Marcello Massimini, Melanie Boly, and Giulio Tononi. 2016. <span>“Neural Correlates of Consciousness: Progress and Problems.”</span> <em>Nature Reviews Neuroscience</em> 17 (5): 307–21.
</div>
<div id="ref-kouider2013neural" class="csl-entry" role="doc-biblioentry">
Kouider, Sid, Carsten Stahlhut, Sofie V Gelskov, Leonardo S Barbosa, Michel Dutat, Vincent de Gardelle, Anne Christophe, Stanislas Dehaene, and Ghislaine Dehaene-Lambertz. 2013. <span>“A Neural Marker of Perceptual Consciousness in Infants.”</span> <em>Science</em> 340 (6130): 376–80.
</div>
<div id="ref-lavazza2021consciousnessoids" class="csl-entry" role="doc-biblioentry">
Lavazza, Andrea. 2021. <span>“<span><span>‘Consciousnessoids’</span></span>: Clues and Insights from Human Cerebral Organoids for the Study of Consciousness.”</span> <em>Neuroscience of Consciousness</em> 2021 (2): niab029.
</div>
<div id="ref-mansour2018vivo" class="csl-entry" role="doc-biblioentry">
Mansour, Abed AlFatah, J Tiago Gonçalves, Cooper W Bloyd, Hao Li, Sarah Fernandes, Daphne Quang, Stephen Johnston, Sarah L Parylak, Xin Jin, and Fred H Gage. 2018. <span>“An in Vivo Model of Functional and Vascularized Human Brain Organoids.”</span> <em>Nature Biotechnology</em> 36 (5): 432–41.
</div>
<div id="ref-metzinger2021artificial" class="csl-entry" role="doc-biblioentry">
Metzinger, Thomas. 2021. <span>“Artificial Suffering: An Argument for a Global Moratorium on Synthetic Phenomenology.”</span> <em>Journal of Artificial Intelligence and Consciousness</em> 8 (01): 43–66.
</div>
<div id="ref-nagel1980like" class="csl-entry" role="doc-biblioentry">
Nagel, Thomas. 1980. <span>“What Is It Like to Be a Bat?”</span> In <em>The Language and Thought Series</em>, 159–68. Harvard University Press.
</div>
<div id="ref-oderberg1993metaphysics" class="csl-entry" role="doc-biblioentry">
Oderberg, David. 1993. <em>The Metaphysics of Identity over Time</em>. Springer.
</div>
<div id="ref-overgaard2012kinds" class="csl-entry" role="doc-biblioentry">
Overgaard, Morten, and Kristian Sandberg. 2012. <span>“Kinds of Access: Different Methods for Report Reveal Different Kinds of Metacognitive Access.”</span> <em>Philosophical Transactions of the Royal Society B: Biological Sciences</em> 367 (1594): 1287–96.
</div>
<div id="ref-perez2023towards" class="csl-entry" role="doc-biblioentry">
Perez, Ethan, and Robert Long. 2023. <span>“Towards Evaluating <span>AI</span> Systems for Moral Status Using Self-Reports.”</span> <em>arXiv Preprint arXiv:2311.08576</em>.
</div>
<div id="ref-plotnik2006self" class="csl-entry" role="doc-biblioentry">
Plotnik, Joshua M, Frans BM De Waal, and Diana Reiss. 2006. <span>“Self-Recognition in an <span>Asian</span> Elephant.”</span> <em>Proceedings of the National Academy of Sciences</em> 103 (45): 17053–57.
</div>
<div id="ref-reiss2001mirror" class="csl-entry" role="doc-biblioentry">
Reiss, Diana, and Lori Marino. 2001. <span>“Mirror Self-Recognition in the Bottlenose Dolphin: A Case of Cognitive Convergence.”</span> <em>Proceedings of the National Academy of Sciences</em> 98 (10): 5937–42.
</div>
<div id="ref-ricard2016plea" class="csl-entry" role="doc-biblioentry">
Ricard, Matthieu. 2016. <em>A Plea for the Animals: The Moral, Philosophical, and Evolutionary Imperative to Treat All Beings with Compassion</em>. Shambhala Publications.
</div>
<div id="ref-rodkey2013infancy" class="csl-entry" role="doc-biblioentry">
Rodkey, Elissa N, and Rebecca Pillai Riddell. 2013. <span>“The Infancy of Infant Pain Research: The Experimental Origins of Infant Pain Denial.”</span> <em>The Journal of Pain</em> 14 (4): 338–50.
</div>
<div id="ref-schechtman2005personal" class="csl-entry" role="doc-biblioentry">
Schechtman, Marya. 2005. <span>“Personal Identity and the Past.”</span> <em>Philosophy, Psychiatry, &amp; Psychology</em> 12 (1): 9–22.
</div>
<div id="ref-shevlin2021non" class="csl-entry" role="doc-biblioentry">
Shevlin, Henry. 2021. <span>“Non-Human Consciousness and the Specificity Problem: A Modest Theoretical Proposal.”</span> <em>Mind &amp; Language</em> 36 (2): 297–314.
</div>
<div id="ref-smirnova2023organoid" class="csl-entry" role="doc-biblioentry">
Smirnova, Lena, Brian S Caffo, David H Gracias, Qi Huang, Itzy E Morales Pantoja, Bohao Tang, Donald J Zack, et al. 2023. <span>“Organoid Intelligence <span>(OI)</span>: <span>T</span>he New Frontier in Biocomputing and Intelligence-in-a-Dish.”</span> <em>Frontiers in Science</em> 1: 1017235.
</div>
<div id="ref-smith2005locked" class="csl-entry" role="doc-biblioentry">
Smith, Eimear, and Mark Delargy. 2005. <span>“Locked-in Syndrome.”</span> <em>Bmj</em> 330 (7488): 406–9.
</div>
<div id="ref-squire2015conscious" class="csl-entry" role="doc-biblioentry">
Squire, Larry R, and Adam JO Dede. 2015. <span>“Conscious and Unconscious Memory Systems.”</span> <em>Cold Spring Harbor Perspectives in Biology</em> 7 (3): a021667.
</div>
<div id="ref-suliman2009cumulative" class="csl-entry" role="doc-biblioentry">
Suliman, Sharain, Siyabulela G Mkabile, Dylan S Fincham, Rashid Ahmed, Dan J Stein, and Soraya Seedat. 2009. <span>“Cumulative Effect of Multiple Trauma on Symptoms of Posttraumatic Stress Disorder, Anxiety, and Depression in Adolescents.”</span> <em>Comprehensive Psychiatry</em> 50 (2): 121–27.
</div>
<div id="ref-sutton2018reinforcement" class="csl-entry" role="doc-biblioentry">
Sutton, Richard S, and Andrew G Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. MIT press.
</div>
<div id="ref-WhiteChristmasBlackMirror" class="csl-entry" role="doc-biblioentry">
Tibbetts, C., and C. Brooker. 2014. <span>“<span>White Christmas</span>.”</span> <span>Episode in <span>“Black Mirror”</span></span>; Channel 4. <a href="https://www.netflix.com/title/70264888">https://www.netflix.com/title/70264888</a>.
</div>
<div id="ref-tononi2008consciousness" class="csl-entry" role="doc-biblioentry">
Tononi, Giulio. 2008. <span>“Consciousness as Integrated Information: A Provisional Manifesto.”</span> <em>The Biological Bulletin</em> 215 (3): 216–42.
</div>
<div id="ref-trewavas2014plant" class="csl-entry" role="doc-biblioentry">
Trewavas, Anthony. 2014. <em>Plant Behaviour and Intelligence</em>. Oxford: Oxford University Press.
</div>
<div id="ref-trewavas2021awareness" class="csl-entry" role="doc-biblioentry">
———. 2021. <span>“Awareness and Integrated Information Theory Identify Plant Meristems as Sites of Conscious Activity.”</span> <em>Protoplasma</em> 258 (3): 673–79.
</div>
<div id="ref-umbrello2019nonconscious" class="csl-entry" role="doc-biblioentry">
Umbrello, Steven, and Stefan Lorenz Sorgner. 2019. <span>“Nonconscious Cognitive Suffering: Considering Suffering Risks of Embodied Artificial Intelligence.”</span> <em>Philosophies</em> 4 (2): 24.
</div>
<div id="ref-vrselja2019restoration" class="csl-entry" role="doc-biblioentry">
Vrselja, Zvonimir, Stefano G Daniele, John Silbereis, Francesca Talpo, Yury M Morozov, André MM Sousa, Brian S Tanaka, et al. 2019. <span>“Restoration of Brain Circulation and Cellular Functions Hours Post-Mortem.”</span> <em>Nature</em> 568 (7752): 336–43.
</div>
<div id="ref-wang2024comprehensive" class="csl-entry" role="doc-biblioentry">
Wang, Liyuan, Xingxing Zhang, Hang Su, and Jun Zhu. 2024. <span>“A Comprehensive Survey of Continual Learning: Theory, Method and Application.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>.
</div>
<div id="ref-wegner2003mind" class="csl-entry" role="doc-biblioentry">
Wegner, Daniel M. 2003. <span>“The Mind’s Best Trick: <span>H</span>ow We Experience Conscious Will.”</span> <em>Trends in Cognitive Sciences</em> 7 (2): 65–69.
</div>
</div>
</body>
</html>
